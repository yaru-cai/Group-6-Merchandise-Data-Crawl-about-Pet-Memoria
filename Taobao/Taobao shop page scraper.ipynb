{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import third library\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from pyquery import PyQuery as pq\n",
    "import time\n",
    "import openpyxl as op              \n",
    "# Global Variable\n",
    "count = 1                           \n",
    " \n",
    "# Setting browser options\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"excludeSwitches\", ['enable-automation'])\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.maximize_window()\n",
    "wait = WebDriverWait(driver,10)\n",
    "# The web will stop for 10s, login by scaning QR code at this time\n",
    " \n",
    "# Input \"Keyword\", search and begin to scrape\n",
    "def search_goods(KEYWORD,start_page,total_pages):\n",
    "    try:\n",
    "        print('正在搜索: ')\n",
    "        driver.get('https://www.taobao.com')\n",
    "        # time.sleep(10)           \n",
    "        driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\",\n",
    "                               {\"source\": \"\"\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\"\"\"})\n",
    "        # Find input box of \"searching\"\n",
    "        input = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#q\")))\n",
    "        # Find button of \"searching\"\n",
    "        submit = wait.until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, '#J_TSearchForm > div.search-button > button')))\n",
    "        # Input \"Keyword\" to input box\n",
    "        input.send_keys(KEYWORD)\n",
    "        # Click \"searching\" button\n",
    "        submit.click()\n",
    " \n",
    "        # If \"startpage\" is not 1, scroll to the bottom of page manually\n",
    "        if start_page != 1:\n",
    "            time.sleep(3)\n",
    "            # Find input box of \"page\", input \"startpage\"\n",
    "            pageInput = wait.until(EC.presence_of_element_located(\n",
    "                (By.XPATH, '//*[@id=\"pageContent\"]/div[1]/div[3]/div[4]/div/div/span[3]/input')))\n",
    "            pageInput.send_keys(start_page)\n",
    "            # Find and click button of \"turning\"\n",
    "            admit = wait.until(EC.element_to_be_clickable(\n",
    "                (By.XPATH, '//*[@id=\"pageContent\"]/div[1]/div[3]/div[4]/div/div/button[3]')))\n",
    "            admit.click()\n",
    " \n",
    "        # Get data of products\n",
    "        get_goods(start_page)\n",
    " \n",
    "        # Page turning\n",
    "        for i in range(start_page + 1, total_pages+1):\n",
    "            page_turning(i)\n",
    " \n",
    "    except TimeoutException:\n",
    "        print(\"search_goods: error\")\n",
    "        return search_goods(KEYWORD,start_page,total_pages)\n",
    " \n",
    "# Page turning Function\n",
    "def page_turning(page_number):\n",
    "    print('正在翻页: ', page_number)\n",
    "    try:\n",
    "        time.sleep(2)\n",
    "        # Find and click the button of \"next page\"\n",
    "        submit = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"search-content-leftWrap\"]/div[3]/div[4]/div/div/button[2]/span')))\n",
    "        submit.click()\n",
    "        # Judge if the page number correct\n",
    "        wait.until(EC.text_to_be_present_in_element((By.XPATH, '//*[@id=\"search-content-leftWrap\"]/div[3]/div[4]/div/div/span[1]/em'), str(page_number)))\n",
    "        get_goods(page_number)\n",
    "    except TimeoutException:\n",
    "        print(\"page_number: error\")\n",
    "        page_turning(page_number)\n",
    " \n",
    "# Get products data of each page\n",
    "def get_goods(page):\n",
    "    # Claim global varibale\n",
    "    global count\n",
    " \n",
    "    # Wait until whole page load completely by scrolling to bottom\n",
    "    if input('确认界面加载完毕，输入数字“1”开始爬取-->') == 1:\n",
    "        pass\n",
    " \n",
    "    html = driver.page_source\n",
    "    doc = pq(html)\n",
    "    # Extract common parent elements of all items\n",
    "    items = doc('div.contentInner--xICYBlag > a').items()\n",
    " \n",
    "    for item in items:\n",
    "        # Locate title\n",
    "        title = item.find('.title--qJ7Xg_90 span').text()\n",
    "        # Locate price\n",
    "        price_int = item.find('.priceInt--yqqZMJ5a').text()\n",
    "        price_float = item.find('.priceFloat--XpixvyQ1').text()\n",
    "        if price_int and price_float:\n",
    "            price = float(f\"{price_int}{price_float}\")\n",
    "        else:\n",
    "            price = 0.0\n",
    "        # Locate deal\n",
    "        deal = item.find('.realSales--XZJiepmt').text()\n",
    "        # Locate location\n",
    "        location = item.find('.procity--wlcT2xH9 span').text()\n",
    "        # Locate shop\n",
    "        shop = item.find('.shopNameText--DmtlsDKm').text()\n",
    "        # Locate if exemption from postage\n",
    "        postText = item.find('.subIconWrapper--Vl8zAdQn').text()\n",
    "        postText = \"包邮\" if \"包邮\" in postText else \"/\"\n",
    "        # Locate product URL\n",
    "        t_url = item.attr('href')\n",
    "        # Locate shop URL\n",
    "        shop_url = item.find('.TextAndPic--grkZAtsC a')\n",
    "        shop_url = shop_url.attr('href')\n",
    "        # Locate picture URL\n",
    "        img = item.find('.mainPicWrapper--qRLTAeii img')\n",
    "        img_url = img.attr('src')\n",
    "\n",
    "        product = {\n",
    "            'Page': page,\n",
    "            'Num': count-1,\n",
    "            'title': title,\n",
    "            'price': price,\n",
    "            'deal': deal,\n",
    "            'location': location,\n",
    "            'shop': shop,\n",
    "            'isPostFree': postText,\n",
    "            'url': t_url,\n",
    "            'shop_url': shop_url,\n",
    "            'img_url': img_url\n",
    "        }\n",
    "        print(product)\n",
    " \n",
    "        # Write data in Excel\n",
    "        wb.cell(row=count, column=1, value=page)                \n",
    "        wb.cell(row=count, column=2, value=count-1)            \n",
    "        wb.cell(row=count, column=3, value=title)               \n",
    "        wb.cell(row=count, column=4, value=price)               \n",
    "        wb.cell(row=count, column=5, value=deal)                \n",
    "        wb.cell(row=count, column=6, value=location)            \n",
    "        wb.cell(row=count, column=7, value=shop)                \n",
    "        wb.cell(row=count, column=8, value=postText)           \n",
    "        wb.cell(row=count, column=9, value=t_url)               \n",
    "        wb.cell(row=count, column=10, value=shop_url)          \n",
    "        wb.cell(row=count, column=11, value=img_url)           \n",
    "        count += 1                                             \n",
    " \n",
    "# Main function\n",
    "def Crawer_main(KEYWORD,pageStart,pageEnd):\n",
    "    try:\n",
    "        search_goods(KEYWORD,pageStart,pageEnd)\n",
    "    except Exception as exc:\n",
    "        print('Crawer_main函数报错:', exc)\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    " \n",
    "    KEYWORD = input('输入搜索的商品关键词Keyword：')           \n",
    "    pageStart = int(input('输入爬取的起始页PageStart：'))    \n",
    "    pageEnd = int(input('输入爬取的终止页PageEnd：'))        \n",
    " \n",
    "    # Establish Excel file\n",
    "    try:\n",
    "        ws = op.Workbook()                                  \n",
    "        wb = ws.create_sheet(index=0)                  \n",
    "        # Excel第一行：表头\n",
    "        title_list = ['Page', 'Num', 'title', 'Price', 'Deal', 'Location', 'Shop', 'IsPostFree', 'Title_URL',\n",
    "                      'Shop_URL', 'Img_URL']\n",
    "        for i in range(0, len(title_list)):\n",
    "            wb.cell(row=count, column=i + 1, value=title_list[i])\n",
    "        count += 1  # Write data in next line\n",
    "    except Exception as exc:\n",
    "        print(\"Excel建立失败！\")\n",
    " \n",
    "    # Start to scrape\n",
    "    Crawer_main(KEYWORD,pageStart,pageEnd)\n",
    " \n",
    "    # Save Excel file\n",
    "    Filename = input(\"输入存储文件名称：\")\n",
    "    Filename = Filename + '_From_Taobao.xlsx'\n",
    "    ws.save(filename = Filename)\n",
    "    print(Filename + \"存储成功~\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
